"""
Create tar files corresponding to 40 m/pixel tiles combining all the 10 m/pixel, 20
m/pixel, and 40 m/pixel Sentinel-2 images into the tar file.
The Sentinel-2 images are split into 1.25 m/pixel files that contain multiple images
(up to 32).
Then upload the tar files to Hugging Face.
"""
import csv
from datetime import date, timedelta
import glob
import io
import multiprocessing
import os
import random
import tarfile

import affine
import numpy as np
from PIL import Image
import rasterio
from rasterio.crs import CRS
import tqdm

input_dirs = [
    "/mnt/sentinel2_2019_1/tiles/sentinel2",
    "/mnt/sentinel2_2019_2/tiles/sentinel2",
    "/mnt/sentinel2_2020_1/tiles/sentinel2",
    "/mnt/sentinel2_2020_2/tiles/sentinel2",
    "/mnt/sentinel2_2021_1/tiles/sentinel2",
    "/mnt/sentinel2_2021_2/tiles/sentinel2",
]
num_workers = 64
naip_csv_fname = "/mnt/data/naip.csv"
tar_dir = "/mnt/data/sentinel2_tar/"
bands = {
    8: ["B02", "B03", "B04", "B08"],
    16: ["B05", "B06", "B07", "B11", "B12", "B8A"],
    32: ["B01", "B09", "B10"],
}
min_images = 16
max_images = 32

def get_yearmo_offset(yearmo, offset):
    d = date(int(yearmo[0:4]), int(yearmo[4:6]), 15)
    sign = 1
    if offset < 0:
        offset = -offset
        sign = -1
    for _ in range(offset):
        d += sign * timedelta(days=30)
        d = d.replace(day=15)
    return d.strftime("%Y%m")

# Figure out which 1.25 m/pixel tiles, and which year/month for each tile, are needed.
# Group these by big tile (40 m/pixel).
needed_tiles = {}
with open(naip_csv_fname) as f:
    reader = csv.DictReader(f)
    for row in tqdm.tqdm(reader, desc="Reading CSV"):
        small_tile = (row["projection"], int(row["col"]), int(row["row"]))
        big_tile = (small_tile[0], small_tile[1]//32, small_tile[2]//32)
        parts = row["naip_scene"].split("_")
        # Should be parts[5] but we messed up in 3_sentinel2_windows
        # (using processing time instead of sense time).
        assert len(parts[-1]) == 8
        yearmo = parts[-1][0:6]
        if big_tile not in needed_tiles:
            needed_tiles[big_tile] = []
        needed_tiles[big_tile].append((small_tile, yearmo))

# Identify available Sentinel-2 images.
def get_fnames(image_dir):
    fnames = glob.glob(os.path.join(image_dir, "*/*/*.tif"))
    cur_images = {}
    for fname in fnames:
        parts = fname.split("/")
        projection = parts[-2].split("_")[0].split(":")[1]
        tile_parts = parts[-1].split(".")[0].split("_")
        col = int(tile_parts[0])
        row = int(tile_parts[1])
        tile = (projection, col, row)

        band = parts[-3]
        yearmo = parts[-4].split("_")[2][0:6]

        if (band, tile, yearmo) not in cur_images:
            cur_images[(band, tile, yearmo)] = []
        cur_images[(band, tile, yearmo)].append(fname)
    return cur_images

jobs = []
for input_dir in input_dirs:
    for image_name in os.listdir(input_dir):
        jobs.append(os.path.join(input_dir, image_name))
sentinel2_images = {}
p = multiprocessing.Pool(num_workers)
random.shuffle(jobs)
outputs = p.imap_unordered(get_fnames, jobs)
for cur_images in tqdm.tqdm(outputs, total=len(jobs), desc="Get Sentinel-2 filenames"):
    for k, v in cur_images.items():
        if k not in sentinel2_images:
            sentinel2_images[k] = []
        sentinel2_images[k].extend(v)
p.close()

def process(job):
    big_tile, small_tiles = job

    tar_fname = os.path.join(tar_dir, f"{big_tile[0]}_{big_tile[1]}_{big_tile[2]}.tar")
    csv_fname = os.path.join(tar_dir, f"{big_tile[0]}_{big_tile[1]}_{big_tile[2]}.csv")
    metadata = []

    with tarfile.open(tar_fname+".tmp", "w") as tar_file:
        # Use the first band to determine which images to use
        # (only using images with no missing pixels).

        for band_res, band_names in bands.items():
            # Group small tiles at the band resolution.
            small_tiles_by_band_tile = {}
            for small_tile, yearmo in small_tiles:
                band_tile = (small_tile[0], small_tile[1]//band_res, small_tile[2]//band_res)
                if band_tile not in small_tiles_by_band_tile:
                    small_tiles_by_band_tile[band_tile] = []
                small_tiles_by_band_tile[band_tile].append((small_tile, yearmo))

            for band_tile, cur_small_tiles in small_tiles_by_band_tile.items():
                yearmos = set()
                for _, yearmo in cur_small_tiles:
                    for offset in [-2, -1, 0, 1, 2]:
                        yearmos.add(get_yearmo_offset(yearmo, offset))

                # Use the first band to determine which images to use
                # (only using images with no missing pixels).
                first_band = band_names[0]

                images_by_yearmo = {yearmo: [] for yearmo in yearmos}
                for yearmo in yearmos:
                    if (first_band, band_tile, yearmo) not in sentinel2_images:
                        continue

                    for fname in sentinel2_images[(first_band, band_tile, yearmo)]:
                        # Make sure the other bands exist for this image.
                        okay = True
                        for other_band in band_names[1:]:
                            other_fname = fname.replace(f"/{first_band}/", f"/{other_band}/")
                            if os.path.exists(other_fname):
                                continue
                            okay = False
                            break
                        if not okay:
                            continue

                        im = np.array(Image.open(fname))
                        images_by_yearmo[yearmo].append((fname, im))

                # So now select the images for each small tile.
                # And read the images and write the .tifs.
                # We read images into a cache so other small tiles can reuse them.
                image_cache = {fname: im for image_list in images_by_yearmo.values() for fname, im in image_list}
                for small_tile, yearmo in cur_small_tiles:
                    crop_size = 512 // band_res
                    crop_start = (
                        small_tile[1] - band_tile[1]*band_res,
                        small_tile[2] - band_tile[2]*band_res,
                    )
                    candidate_fnames = []
                    for offset in [-2, -1, 0, 1, 2]:
                        cur_yearmo = get_yearmo_offset(yearmo, offset)
                        for fname, im in images_by_yearmo[cur_yearmo]:
                            num_missing = np.count_nonzero(im == 0)
                            num_cloudy = np.count_nonzero(im > 1500)
                            if num_missing > crop_size:
                                continue
                            candidate_fnames.append((fname, num_missing*5+num_cloudy))
                    if len(candidate_fnames) < min_images:
                        continue
                    candidate_fnames.sort(key=lambda t: t[1])
                    fnames = [fname for fname, _ in candidate_fnames[0:max_images]]

                    data = []
                    for index, fname in enumerate(fnames):
                        sentinel2_scene = fname.split("/")[-4]
                        metadata.append({
                            "projection": small_tile[0],
                            "col": small_tile[1],
                            "row": small_tile[2],
                            "index": index,
                            "scene": sentinel2_scene,
                        })

                        for band in band_names:
                            cur_fname = fname.replace(f"/{first_band}/", f"/{band}/")
                            if cur_fname not in image_cache:
                                image_cache[cur_fname] = np.array(Image.open(cur_fname))
                            crop = image_cache[cur_fname][crop_start[1]*crop_size:(crop_start[1]+1)*crop_size, crop_start[0]*crop_size:(crop_start[0]+1)*crop_size]
                            data.append(crop)
                    data = np.stack(data, axis=0)

                    crs = CRS.from_epsg(int(small_tile[0]))
                    transform = affine.Affine(
                        1.25 * band_res,
                        0,
                        small_tile[1] * crop_size * 1.25 * band_res,
                        0,
                        -1.25 * band_res,
                        small_tile[2] * crop_size * -1.25 * band_res,
                    )
                    profile = {
                        "driver": "GTiff",
                        "compress": "lzw",
                        "width": data.shape[2],
                        "height": data.shape[1],
                        "count": data.shape[0],
                        "dtype": data.dtype.name,
                        "crs": crs,
                        "transform": transform,
                    }
                    buf = io.BytesIO()
                    with rasterio.open(buf, "w", **profile) as dst:
                        dst.write(data)
                    out_entry = tarfile.TarInfo(name=f"sentinel2/{small_tile[0]}_{small_tile[1]}_{small_tile[2]}_{band_res}.tif")
                    out_entry.size = buf.getbuffer().nbytes
                    out_entry.mode = 0o644
                    buf.seek(0)
                    tar_file.addfile(out_entry, fileobj=buf)

    os.rename(tar_fname+".tmp", tar_fname)

    with open(csv_fname, "w") as f:
        writer = csv.DictWriter(f, ["projection", "col", "row", "index", "scene"])
        writer.writeheader()
        writer.writerows(metadata)

jobs = list(needed_tiles.items())
random.shuffle(jobs)
p = multiprocessing.Pool(num_workers)
outputs = p.imap_unordered(process, jobs)
for _ in tqdm.tqdm(outputs, total=len(jobs), desc="Process"):
    pass
p.close()
